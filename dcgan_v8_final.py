# -*- coding: utf-8 -*-
"""DCGAN_V8_final2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rZsJMJt6tPHNpgwDGWGxIRD08BmQ11I1

## Import Library
"""

pip install d2l

#from d2l import torch as d2l
import torch
import torchvision
from torch import nn
import warnings
from PIL import Image
from d2l import torch as d2l
from __future__ import print_function
from enum import Enum
import argparse
import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML
import pandas as pd
import math
import time
import datetime

from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import functools
import torch.nn.functional as F

"""## Set random seed"""

# Set random seed
manualSeed = 999
print("Random Seed: ", manualSeed)
random.seed(manualSeed)
torch.manual_seed(manualSeed)

"""## Data Loading"""

from google.colab import drive
drive.mount('/content/drive')

"""###Creating Input Image with a given WAP location"""

def HSL_to_RGB(scaled, out_range_indicator):
  hue = ((scaled)**3) * 240
  if out_range_indicator == True:
    saturation = 0.35
    luminance = 0.35
  else:
    saturation = 1
    luminance = 0.6
  c = (1 - abs(2 * luminance - 1)) * saturation
  x = c * (1 - abs((hue / 60) % 2 - 1))
  m = luminance - c/ 2
  r_ = 0
  g_ = 0
  b_ = 0
  if hue < 60:
    (r_, g_, b_) = (c,x,0)
  elif hue < 120:
    (r_, g_, b_) = (x,c,0)
  elif hue < 180:
    (r_, g_, b_) = (0,c,x)
  elif hue < 240:
    (r_, g_, b_) = (0,x,c)
  elif hue < 300:
    (r_, g_, b_) = (x,0,c)
  else:
    (r_, g_, b_) = (c,0,x)
  R = int((r_ + m) * 255)
  G = int((g_ + m) * 255)
  B = int((b_ + m) * 255)
  return (R,G,B)

"""#### Building the coordinate to 36*36 index conversion scale"""

header2 = ['id(row)', 'id(col)','X[m]',	'Y[m]',	'Z[m]']
map = pd.read_csv(
      "/content/drive/MyDrive/ECE496 Capstone/data/coordinate determination/Please Follow This.csv",
      names=header2,
      index_col=False)
#map = pd.read_csv(
#      "/content/drive/MyDrive/data/coordinate determination/Please Follow This.csv",
#      names=header2,
#      index_col=False)
map = map[1:]
map_np=np.array(map)
map_np = map_np.astype(np.float)
map_np_test = map_np[:1296,:4]
map_np_limited = map_np[:1296,:4]
#print(map_np_test[850:901,:])
map_np_limited[:,2] -= 0.3125
map_np_limited[:,2] /= 0.625
map_np_limited[:,3] -= 0.3125
map_np_limited[:,3] /= 0.625
map_np_limited[:,2] += 55
map_np_limited[:,3] += 73
conversion_map = np.zeros((128,128,2))
conversion_map -= 1
for entries in map_np_limited:
  (x,y) = (int(entries[2]),int(entries[3]))
  conversion_map[x,y,0] = int(entries[0])
  conversion_map[x,y,1] = int(entries[1])
#print(map_np_limited[850:901,:])

"""#### Inputting Image Generation"""

def creating_input(x,y, n = 36):
  input_map = np.zeros((36,36,3))
  (R_i,G_i,B_i) = HSL_to_RGB(0, True)
  input_map[:,:,0] = R_i
  input_map[:,:,1] = G_i
  input_map[:,:,2] = B_i
  input_map /= 255
  pixel_x = math.floor((x)/0.625 + 55)
  pixel_y = math.floor((y)/0.625 + 73)
  #print(pixel_x,pixel_y)
  (ind_x,ind_y) = conversion_map[pixel_x,pixel_y]
  if (ind_x,ind_y) == (-1,-1):
    print("Out of range, base_map is shown")
    return input_map
  if ind_x < n and ind_y < n:
    ind_x = int(ind_x)
    ind_y = int(ind_y)
    #print(ind_x, ind_y)
    (R,G,B) = HSL_to_RGB(1, False)
    input_map[ind_x,ind_y,0] = R/255
    input_map[ind_x,ind_y,1] = G/255
    input_map[ind_x,ind_y,2] = B/255

  return input_map

def creating_input_with_mask(x,y, n = 36):
  input_map = np.zeros((36,36,3))
  (R_i,G_i,B_i) = HSL_to_RGB(0, True)
  input_map[:,:,0] = R_i
  input_map[:,:,1] = G_i
  input_map[:,:,2] = B_i
  input_map /= 255
  pixel_x = math.floor((x)/0.625 + 55)
  pixel_y = math.floor((y)/0.625 + 73)
  #print(pixel_x,pixel_y)
  (ind_x,ind_y) = conversion_map[pixel_x,pixel_y]
  if (ind_x,ind_y) == (-1,-1):
    print("Out of range, base_map is shown")
    return input_map
  if ind_x < n and ind_y < n:
    ind_x = int(ind_x)
    ind_y = int(ind_y)
    #print(ind_x, ind_y)
    (R,G,B) = HSL_to_RGB(1, False)
    input_map[ind_x,ind_y,0] = R/255
    input_map[ind_x,ind_y,1] = G/255
    input_map[ind_x,ind_y,2] = B/255

  mask = np.zeros((36,36), dtype=bool)
  mask [ind_x,ind_y] = True
  return input_map, mask

def label_correspond(x,y):
  pixel_x = math.floor((x)/0.625 + 55)
  pixel_y = math.floor((y)/0.625 + 73)
  #print(pixel_x, pixel_y)
  correspond_x = str(float(pixel_x - 55) * 0.625 + 0.3125)
  correspond_y = str(float(pixel_y - 73) * 0.625 + 0.3125)
  label = correspond_x + "_" +   correspond_y + "_2.9"
  #print(label)
  return label

map_test = creating_input(-19.375,-43.125)
label = label_correspond(-19.375,-43.125)
plt.imshow(map_test)
print(label)

"""### Dataset Structure"""

# Custom Dataset Class
class CustomDataset(Dataset):
    def __init__(self, image_dir, transform):
        self.image_dir = image_dir
        self.transform = transform
        self.image_names = os.listdir(image_dir)
        self.image_names.sort()    

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        image_names = os.path.join(self.image_dir, self.image_names[idx])
        image = Image.open(image_names)

        if self.transform:
            image = self.transform(image)

        return image

# Create CustomDataset instance
dataloader_path = '/content/drive/MyDrive/ECE496 Capstone/data/1296 dataset (no duplicate)/image_new_encode/'
dataset = CustomDataset(dataloader_path, None)
# images_o = torch.from_numpy(np.array(dataset[0])) #convert from PIL --> tensor

"""
### Combined List for input and data"""

def input_with_mask_channel(mask, input_image):
    """
    Returns 4 channel image with binary mask as 4th channel
    mask - binary mask (tensor)
    image - tensor of image (tensor)
    """

    # Stack mask as 4th channel
    input_image_with_mask = torch.from_numpy(np.dstack((input_image,mask)))
    input_image_with_mask = input_image_with_mask.permute(2,0,1)
    if list(input_image_with_mask.shape) != [4, 36, 36]:
      print(input_image_with_mask.shape)
    input_image_with_mask = np.array(input_image_with_mask)
    # Output - 4 x 36 x 36
    return input_image_with_mask

def get_physical_coordinates(name):
    return [float(s) for s in name.strip('.png').split("_")]

def create_combined_list(data_path=dataloader_path, image_size=36, dataset=dataset):
    """
    Returns a nested list with length equal to number of images found in folder dataloader_path 'dataloader_path'
    Each index has 2 nested elements:
      idx[0]: transformed pixel image data
      idx[1]: input images for generator

    dataset is an instance of the InpaintingDataset __getitem__ iterable method
    """
    pixel_image_data = []
    generator_input_image = []
    count = 0
    for images in dataset:
        if count % 50 == 0:
            print(f'image number {count} processed')

        pixel_image = np.array(images)[0:image_size,0:image_size,0:3]
        pixel_image = torch.from_numpy(pixel_image) #convert from PIL --> tensor
        #print(pixel_image.shape)
        pixel_image = pixel_image.permute(2,0,1)
        
        pixel_image = pixel_image.float()
        #print(pixel_image[:][:][0])
        #input('Press Enter...')
        # Transformed pixel image data
        pixel_image_data.append(pixel_image)

        # Input images for generator
        coordinates = get_physical_coordinates(dataset.image_names[count])
        input_image, mask = creating_input_with_mask(coordinates[0],coordinates[1])
        input_image_masked = input_with_mask_channel(mask, input_image)
        input_image_tensor = torch.from_numpy(input_image_masked)
        #input_image_tensor = input_image_tensor.permute(2,0,1)
        input_image_tensor = input_image_tensor.float()
        #print(input_image_tensor.shape)
        #input('Press Enter...')
        generator_input_image.append(input_image_tensor)
        
        count += 1

    # Create nested empty list
    dataset_combined = [ [ [] for i in range(2)] for i in range(len(dataset)) ]

    for idx in range(len(dataset)):
        dataset_combined[idx][0] = pixel_image_data[idx]
        dataset_combined[idx][1] = generator_input_image[idx]

    print(dataset_combined[0][0].shape)

    print('Completed')

    return dataset_combined

dataset_combined = create_combined_list(data_path=dataloader_path, image_size=36, dataset=dataset)

"""### Dataset Splitting"""

def data_loader(bs = 8, t_ind = 0.8, v_ind = 0.1, o_num = 2, dataset = dataset_combined):
    #start_time = time.time()
    
    loader = []
    t_end = int(len(dataset) * t_ind)
    v_end = int(len(dataset) * (t_ind + v_ind))
    
    np.random.seed(1500)  # Fixed numpy random seed for reproducible shuffling
    np.random.shuffle(dataset)
    data_split = [
        dataset[:t_end]
        , dataset[t_end:v_end]
        , dataset[v_end:]
        , dataset[:o_num]
    ]
    print(len(dataset))
    load_order = ['training data', 'validation data', 'testing data', 'overfitting']
    order = 0
    for split in data_split:
        print(f"Loading images for {load_order[order]}. Images to be loaded: {len(data_split[order])}")
        loader.append(DataLoader(split, batch_size=bs, shuffle = True))
        order += 1

    #end_time = time.time()
    #elapsed_time = end_time - start_time
    #print("Total Time Elapsed: {:.2f} seconds".format(elapsed_time))

    return tuple(loader)

bs = 1
t_ind = 0.8
v_ind = 0.1
o_num = 1
print(dataset_combined[0][0].shape)
train_dataset, valid_dataset, test_dataset, overfit_dataset = data_loader(bs = bs, t_ind = t_ind, v_ind = v_ind, o_num = o_num, dataset = dataset_combined)
dataiter = iter(train_dataset)
inputs, labels = dataiter.next()
print(inputs.shape)
print(labels.shape)

"""## Model Defination

### Define Default Generator
"""

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=2, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(True),
            nn.Conv2d(in_channels=32,  out_channels=64, kernel_size=2, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.Conv2d(in_channels=64,  out_channels=128, kernel_size=2, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.Conv2d(in_channels=128,  out_channels=256, kernel_size=2, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(in_channels=256,  out_channels=128, kernel_size=2, stride=2, padding=1, output_padding=0),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(in_channels=128,  out_channels=64, kernel_size=2, stride=2, padding=1, output_padding=0),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(in_channels=64,  out_channels=32, kernel_size=2, stride=1, padding=1, output_padding=0),
            nn.BatchNorm2d(32),
            nn.ReLU(True),
            nn.ConvTranspose2d(in_channels=32,  out_channels=3, kernel_size=2, stride=1, padding=1, output_padding=0),
            nn.Tanh()
        )

    def forward(self, x):
        #print("Input:", x.shape)
        x = self.encoder(x)
        #print("Encoder:", x.shape)
        x = self.decoder(x)
        #print("Decoder:", x.shape)
        return x

"""### Define U Net Generator


"""

class UNetGenerator(nn.Module):
    def __init__(self):
        super(UNetGenerator, self).__init__()

        self.in_conv = DoubleConv(input_channels = 4, output_channels = 8)
        self.down_seq_1 = DownSequence(input_channels = 8, output_channels = 16)
        self.down_seq_2 = DownSequence(input_channels = 16, output_channels = 32)
        self.down_seq_3 = DownSequence(input_channels = 32, output_channels = 64)
        self.down_seq_4 = DownSequence(input_channels = 64, output_channels = 128)
        self.down_seq_5 = DownSequence(input_channels = 128, output_channels = 256)
        self.down_seq_6 = DownSequence(input_channels = 256, output_channels = 512)
        self.up_seq_6 = UpSequence(input_channels = 512, output_channels = 256)
        self.up_seq_5 = UpSequence(input_channels = 256, output_channels = 128)
        self.up_seq_4 = UpSequence(input_channels = 128, output_channels = 64)
        self.up_seq_3 = UpSequence(input_channels = 64, output_channels = 32)
        self.up_seq_2 = UpSequence(input_channels = 32, output_channels = 16)
        self.up_seq_1 = UpSequence(input_channels = 16, output_channels = 8)
        self.out_conv = OutConv(input_channels = 8, output_channels = 3)
        
    def forward(self, x):
        x_dc = self.in_conv(x)
        #print('dc: x_dc.shape', x_dc.shape)
        x_d1 = self.down_seq_1(x_dc)
        #print('d1: x_d1.shape', x_d1.shape)
        x_d2 = self.down_seq_2(x_d1)
        #print('d2: x_d2.shape', x_d2.shape)
        x_d3 = self.down_seq_3(x_d2)
        #print('d3: x_d3.shape', x_d3.shape)
        x_d4 = self.down_seq_4(x_d3)
        #print('d4: x_d4.shape', x_d4.shape)
        x_d5 = self.down_seq_5(x_d4)
        #print('d5: x_d5.shape', x_d5.shape)
        x_d6 = self.down_seq_6(x_d5)
        #print('d6: x_d6.shape', x_d6.shape)
        x_u6 = self.up_seq_6(x_d5, x_d6)
        #print('u6: x_u6.shape', x_u6.shape)
        x_u5 = self.up_seq_5(x_d4, x_d5)
        #print('u5: x_u5.shape', x_u5.shape)
        x_u4 = self.up_seq_4(x_d3, x_d4)
        #print('u4: x_u4.shape', x_u4.shape)
        x_u3 = self.up_seq_3(x_d2, x_u4)
        #print('u3: x_u3.shape', x_u3.shape)
        x_u2 = self.up_seq_2(x_d1, x_u3)
        #print('u2: x_u2.shape', x_u2.shape)
        x_u1 = self.up_seq_1(x_dc, x_u2)
        #print('u1: x_u1.shape', x_u1.shape)
        return self.out_conv(x_u1)

class DoubleConv(nn.Module):
    def __init__(self, input_channels, output_channels, mid_channels = None):
        super(DoubleConv, self).__init__()
        if mid_channels == None:
            mid_channels = output_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels = input_channels, 
                      out_channels = mid_channels, 
                      kernel_size = 3,
                      stride = 1, 
                      padding = 1
                      ),
            nn.BatchNorm2d(mid_channels),
            nn.LeakyReLU(0.2,True),
            nn.Conv2d(in_channels = mid_channels, 
                      out_channels = output_channels, 
                      kernel_size = 3, 
                      stride = 1, 
                      padding = 1
                      ),
            nn.BatchNorm2d(output_channels),
            nn.LeakyReLU(0.2,True)
        )

    def forward(self, x):
        #print('at DoubleConv')
        return self.double_conv(x)

class DownSequence(nn.Module):
    def __init__(self, input_channels, output_channels):
        super(DownSequence, self).__init__()
        self.down_seq = nn.Sequential(
            nn.MaxPool2d(kernel_size=2, stride = 2, padding = 1),
            DoubleConv(input_channels, output_channels)
        )

    def forward(self, x):
        #print('at down_seq')
        return self.down_seq(x)

class UpSequence(nn.Module):
    def __init__(self, input_channels, output_channels):
        super(UpSequence, self).__init__()
        self.up_seq = nn.ConvTranspose2d(input_channels , 
                                         input_channels // 2, 
                                         kernel_size=2, 
                                         stride = 2,
                                         )
        self.double_conv = DoubleConv(input_channels, 
                                      output_channels)

    def forward(self, x_d, x_u):
        #print('at up_seq')
        x_u = self.up_seq(x_u)
        # selects only size of x_u data from x_d
        diffY = x_d.size()[2] - x_u.size()[2]
        diffX = x_d.size()[3] - x_u.size()[3]
        #print('x_u.shape before cutting', x_u.shape)
        x_u = F.pad(x_u, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])
        #print('x_u.shape after cutting', x_u.shape)
        
        #print('x_d.shape', x_d.shape)
        return self.double_conv(torch.cat([x_d, x_u], 1))

class OutConv(nn.Module):
    def __init__(self, input_channels, output_channels):
        super(OutConv, self).__init__()
        self.out_conv = nn.Conv2d(input_channels, 
                                  output_channels, 
                                  kernel_size=1)

    def forward(self, x):
        return self.out_conv(x)

dataiter = iter(train_dataset)
sample = dataiter.next()
sample = sample[1]
sample = sample

model_G = UNetGenerator()
output = model_G(sample)
print("Input:", sample.shape)
print("Output:", output.shape)

"""### Define Discriminator"""

class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    self.main = nn.Sequential (
        nn.Conv2d(in_channels=3,  out_channels=8, kernel_size=2, stride=2, padding=0),
        #nn.BatchNorm2d(8),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=2, stride=2, padding=0),
        nn.BatchNorm2d(16),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, stride=2, padding=0),
        nn.BatchNorm2d(32),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2, padding=0),
        nn.BatchNorm2d(64),
        nn.LeakyReLU(0.2, inplace=True)
    )
    self.fc1 = nn.Sequential (
        nn.Linear(256, 128),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Linear(128,1),
        nn.Sigmoid()
    )

  def forward(self, x):
    x = self.main(x)
    x = x.view(-1, 256)
    x = self.fc1(x)
    return x

"""### Weight Initializations"""

# custom weights initialization called on netG and netD
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv2d') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

"""## Model Training

### Plot curves function
"""

def plot_training_curves(train_acc_csv_path=None, train_loss_csv_path=None,
                         valid_acc_csv_path=None, valid_loss_csv_path=None):
    import matplotlib.pyplot as plt

    # plot 1 - Training vs Validation Accuracy
    if train_acc_csv_path or valid_acc_csv_path:
        if train_acc_csv_path:
            train_acc = np.loadtxt(train_acc_csv_path, delimiter=',').transpose()
            plt.plot(train_acc[0], train_acc[1], label="Train")

        if valid_acc_csv_path:
            val_acc = np.loadtxt(valid_acc_csv_path, delimiter=',').transpose()
            plt.plot(val_acc[0], val_acc[1], label="Validation")

        plt.title("Training Curve - Accuracy")
        plt.xlabel("Iterations")
        plt.ylabel("Accuracy")
        plt.legend(loc='best')

        plt.show()

    # plot 2 - Training vs Validation Loss
    if train_loss_csv_path or valid_loss_csv_path:
        if train_loss_csv_path:
            train_loss = np.loadtxt(train_loss_csv_path, delimiter=',').transpose()
            plt.plot(train_loss[0], train_loss[1], label="Train")

        if valid_loss_csv_path:
            val_loss = np.loadtxt(valid_loss_csv_path, delimiter=',').transpose()
            plt.plot(val_loss[0], val_loss[1], label="Validation")

        plt.title("Training Curve - Loss")
        plt.xlabel("Iterations")
        plt.ylabel("Loss")
        plt.legend(loc='best')

        plt.show()

"""### Loss Function & Optimizer

#### Loss
"""

def generate_binary_label(dlabel, n, device):
    if dlabel.value == 0:
        return torch.zeros([n, 1], device=device)
    elif dlabel.value == 1:
        return torch.ones([n, 1], device=device)

def compute_G_loss(G_model, D_model, criterion, dataset, is_use_cuda, device):
    i = 0
    total_loss = 0.0
    
    for i, data in enumerate(dataset, 0):

        _, input_imgs = data

        input_imgs = input_imgs * 255

        if is_use_cuda and torch.cuda.is_available():
            input_imgs = input_imgs.cuda()

        # get output from generator
        G_output = G_model(input_imgs)
        G_output = (G_output * 127.5) + 127.5

        # get output from discriminator
        D_output = D_model(G_output)

        # get loss for each output and the total loss
        loss = criterion(D_output, generate_binary_label(DLabel.REAL, len(D_output), device))
        total_loss += loss.item()
    
    # get average loss from total loss
    total_loss = float(total_loss) / (i+1)
    return total_loss


def compute_D_loss(G_model, D_model, criterion, dataset, is_use_cuda, device):
    i = 0
    total_loss = 0.0

    for i, data in enumerate(dataset, 0):

        label_imgs, input_imgs = data

        input_imgs = input_imgs * 255

        if is_use_cuda and torch.cuda.is_available():
            input_imgs = input_imgs.cuda()
            label_imgs = label_imgs.cuda()

        # get output images from generator
        G_output = G_model(input_imgs) 
        G_output = (G_output * 127.5) + 127.5

        # get loss for real images
        output_real = D_model(label_imgs)
        loss_real = criterion(output_real, generate_binary_label(DLabel.REAL, len(output_real), device))

        # get loss for fake images
        output_fake = D_model(G_output)
        loss_fake = criterion(output_fake, generate_binary_label(DLabel.FAKE, len(output_fake), device))

        # get total loss
        D_loss = loss_real.item() + loss_fake.item()
        total_loss += D_loss
        
    # get average loss from total loss
    total_loss = float(total_loss) / (i+1)
    return total_loss

"""#### Accuracy"""

def compute_G_accuracy(G_model, D_model, dataset, is_use_cuda):
    acc, total = 0, 0
    for i, data in enumerate(dataset, 0):

        _, input_imgs = data

        input_imgs = input_imgs * 255

        if is_use_cuda and torch.cuda.is_available():
            input_imgs = input_imgs.cuda()

        # get output from generator
        G_output = G_model(input_imgs)
        G_output = (G_output * 127.5) + 127.5

        # get classification result from  discriminator
        D_output = D_model(G_output)
        D_output = D_output.detach().cpu().numpy()

        # calculate acc for image classification
        for i in range(D_output.shape[0]):
            acc += int(D_output[i] > 0.5)
            total += 1
    
    return acc / total


def compute_D_accuracy(G_model, D_model, dataset, is_use_cuda):
    acc_real, acc_fake, total_real, total_fake, total_acc = 0, 0, 0 ,0, 0

    for i, data in enumerate(dataset, 0):

        label_imgs, input_imgs = data

        input_imgs = input_imgs * 255

        if is_use_cuda and torch.cuda.is_available():
            input_imgs = input_imgs.cuda()
            label_imgs = label_imgs.cuda()

        # get output from model 
        G_output = G_model(input_imgs)
        G_output = (G_output * 127.5) + 127.5

        # get loss for real images
        output_real = D_model(label_imgs)
        numpy_output_real = output_real.detach().cpu().numpy()
        # calculate acc for real image classification
        for i in range(numpy_output_real.shape[0]):
            acc_real += int(numpy_output_real[i] > 0.5)
            total_real += 1

        # get loss for fake images
        output_fake = D_model(G_output)
        numpy_output_fake = output_fake.detach().cpu().numpy()
        # calculate acc for fake image classification
        for i in range(numpy_output_fake.shape[0]):
            acc_fake += int(numpy_output_fake[i] < 0.5)
            total_fake += 1

    # calculate total accuracy
    total_acc = (acc_real + acc_fake) / (total_real + total_fake)
    return total_acc

"""####P2P Loss and Accuracy"""

# pixel by pixel loss 
def pixel_loss(original_img, generated_img):
  """ Returns the loss from pixel by pixel comaparison for one image.
      orginal_img - the img from dataset
      generated_img - the img produced by generator
  """

  difference = (original_img - generated_img)**2

  
  diff = (torch.sum(difference))

  return diff.item()

def batch_pixel_loss(original_imgs, generated_imgs):
  """ Returns the loss from pixel by pixel comaparison for one batch.
      orginal_img - the batch of imgs from dataset
      generated_img - the batch of imgs produced by generator
  """

  loss = 0
  for i in range(original_imgs.shape[0]):
    loss += pixel_loss(original_imgs[i], generated_imgs[i])
  
  return loss/original_imgs.shape[0]


# pixel by pixel accuracy 
def pixel_accuracy(original_img, generated_img):
  """ Returns the accuracy from pixel by pixel comaparison for one image.
      orginal_img - the img from dataset
      generated_img - the img produced by generator
  """
  difference = original_img - generated_img
  difference = torch.abs(difference)
  


  diff = 1 - (torch.sum(difference)/(3*36*36))

  # if diff.item() < 0:
  #   print('original_img =', original_img)
  #   print('generated_img =', generated_img)
  #   print('difference =', difference)
  #   print('difference.shape =', difference.shape)
  #   print('diff.item() = ', diff.item())
  #   input('Enter')

  return diff.item()

def batch_pixel_accuracy(original_imgs, generated_imgs):
  """ Returns the accuracy from pixel by pixel comaparison for one batch.
      orginal_img - the batch of imgs from dataset
      generated_img - the batch of imgs produced by generator
  """

  accuracy = 0
  for i in range(original_imgs.shape[0]):
    accuracy += pixel_accuracy(original_imgs[i], generated_imgs[i])
  
  return accuracy/original_imgs.shape[0]

def compute_pixel_loss_accuracy(G_model, dataset, is_use_cuda):
    loss, acc, total = 0, 0, 0
    for i, data in enumerate(dataset, 0):
        label_imgs, input_imgs = data

        if is_use_cuda and torch.cuda.is_available():
            label_imgs = label_imgs.cuda()
            input_imgs = input_imgs.cuda()
        
        input_imgs = input_imgs * 255
        #print('label_imgs =', label_imgs)
        #print('input_imgs =', input_imgs)
        
        # get output from generator
        G_output = G_model(input_imgs)
        G_output = (G_output * 127.5) + 127.5
        #print('G_output =', G_output)
 
        loss += batch_pixel_loss(label_imgs/255, G_output/255)
        acc += batch_pixel_accuracy(label_imgs/255, G_output/255)
        total += 1

    return (loss/total, acc/total)

"""### Training Loop"""

class DLabel(Enum):
    FAKE = 0
    REAL = 1

def train_dcgan(g_model, d_model, train_dataset, valid_dataset, nepochs, id,
                g_criterion=nn.BCELoss(), d_criterion=nn.BCELoss(),
                g_niter_freeze=None, d_niter_freeze=None,
                g_nupdate_per_iter=None, d_nupdate_per_iter=None,
                adam_g_lrate=None, adam_g_betas=None, adam_g_eps=None,
                adam_g_weight_decay=None, adam_g_amsgrad=None,
                adam_d_lrate=None, adam_d_betas=None, adam_d_eps=None,
                adam_d_weight_decay=None, adam_d_amsgrad=None,
                nbatches_per_dpt=None, nbatches_per_plot_g_out=None,
                nepochs_per_model_save=None,
                nimgs_per_plot_g_out=None,
                model_dir=None, is_use_cuda=None):

    device = 'cuda' if is_use_cuda else 'cpu'

    #===================Initial set-up for default arguments===================

    # get batch size
    batch_size = None
    for e in train_dataset:
        batch_size = len(e[0][0])
        break

    # keep track of time elapsed
    start_time = time.time()

    # used for balancing between D and G during training
    if g_niter_freeze is None:
        # num of iterations G freeze
        g_niter_freeze = 0
    if d_niter_freeze is None:
        # num of iterations D freeze
        d_niter_freeze = 0

    # used to increase num of training iteration for G in each epoch
    if g_nupdate_per_iter is None:
        g_nupdate_per_iter = 1
    if d_nupdate_per_iter is None:
        d_nupdate_per_iter = 1

    # parameters for adam optimizer for G 
    if adam_g_lrate is None:
        adam_g_lrate = 0.001
    if adam_g_betas is None:
        adam_g_betas = (0.9, 0.999)
    if adam_g_eps is None:
        adam_g_eps = 1e-8
    if adam_g_weight_decay is None:
        adam_g_weight_decay = 0
    if adam_g_amsgrad is None:
        adam_g_amsgrad = False

    # parameters for adam optimizer for D
    if adam_d_lrate is None:
        adam_d_lrate = 0.001
    if adam_d_betas is None:
        adam_d_betas = (0.9, 0.999)
    if adam_d_eps is None:
        adam_d_eps = 1e-8
    if adam_d_weight_decay is None:
        adam_d_weight_decay = 0
    if adam_d_amsgrad is None:
        adam_d_amsgrad = False

    if nbatches_per_plot_g_out is None:
        nbatches_per_plot_g_out = 25
    if nbatches_per_dpt is None:
        nbatches_per_dpt = 5
    if nepochs_per_model_save is None:
        nepochs_per_model_save = 50
    if nimgs_per_plot_g_out is None:
        nimgs_per_plot_g_out = min(10, batch_size)
    else:
        nimgs_per_plot_g_out = min(nimgs_per_plot_g_out, batch_size)
    if model_dir is None:
        model_dir = "trained_models"
    if is_use_cuda is None:
        is_use_cuda = False

    # store hyperparameter values to txt file
    # create model directory if does not exist
    os.makedirs(model_dir, exist_ok=True)
    with open(rf"{model_dir}\{id}_hyperparameter_specs.txt", 'w') as file:
        file.write(f"timestamp: {datetime.datetime.now()}")
        file.write(f"\n")
        file.write(f"model id: {id}\n")
        file.write(f"batch size: {batch_size}\n")
        file.write(f"number of batches: {len(train_dataset)}\n")
        file.write(f"number of epochs: {nepochs}\n")
        file.write(f"number of batches per datapoint: {nbatches_per_dpt}\n")
        file.write(f"number of batches per plot generator output: {nbatches_per_plot_g_out}\n")
        file.write(f"number of epochs per model save: {nepochs_per_model_save}\n")
        file.write(f"number of images per plot generator output: {nimgs_per_plot_g_out}\n")
        file.write(f"\n")
        file.write(f"GENERATOR\n")
        file.write(f"\t number of iterations to freeze: {g_niter_freeze}\n")
        file.write(f"\t number of updates per iteration: {g_nupdate_per_iter}\n")
        file.write(f"\t learning rate: {adam_g_lrate}\n")
        file.write(f"\t betas: {adam_g_betas}\n")
        file.write(f"\t eps: {adam_g_eps}\n")
        file.write(f"\t weight decay: {adam_g_weight_decay}\n")
        file.write(f"\t amsgrad: {adam_g_amsgrad}\n")
        file.write(f"\n")
        file.write(f"GLOBAL DISCRIMINATOR\n")
        file.write(f"\t number of iterations to freeze: {d_niter_freeze}\n")
        file.write(f"\t number of updates per iteration: {d_nupdate_per_iter}\n")
        file.write(f"\t learning rate: {adam_d_lrate}\n")
        file.write(f"\t betas: {adam_d_betas}\n")
        file.write(f"\t eps: {adam_d_eps}\n")
        file.write(f"\t weight decay: {adam_d_weight_decay}\n")
        file.write(f"\t amsgrad: {adam_d_amsgrad}\n")

    # set Adam optimizer for each model
    g_optimizer = torch.optim.Adam(g_model.parameters(),
                                   lr=adam_g_lrate, betas=adam_g_betas,
                                   eps=adam_g_eps, weight_decay=adam_g_weight_decay,
                                   amsgrad=adam_g_amsgrad)
    d_optimizer = torch.optim.Adam(d_model.parameters(),
                                   lr=adam_d_lrate, betas=adam_d_betas,
                                   eps=adam_d_eps, weight_decay=adam_d_weight_decay,
                                   amsgrad=adam_d_amsgrad)

    # create lists that used for recording information during training
    train_acc_g = list()
    train_loss_g = list()
    val_acc_g = list()
    val_loss_g = list()
    train_acc_d = list()
    train_loss_d = list()
    val_acc_d = list()
    val_loss_d = list()
    train_pixel_acc_g = list()
    train_pixel_loss_g = list()
    val_pixel_acc_g = list()
    val_pixel_loss_g = list()
    niter = nepochs * len(train_dataset)
    iter = 0


    #===========================Training loop starts===========================
    for epoch in range(1, nepochs + 1):
        # iterate through each batch in training dataset
        for label_imgs, input_imgs in train_dataset:
            iter += 1

            # map input image from 0-1 (float) to 0-255 (float)
            input_imgs = input_imgs * 255

            # print("==============================")
            # plt.imshow(np.transpose(input_imgs[0].detach().cpu() / 255,
            #                                 (1, 2, 0)))
            # # plt.imshow(np.transpose(label_imgs[0].detach().cpu() / 255,
            # #                                 (1, 2, 0)))
            # plt.show()

            # print("======================input size:", input_imgs[0].shape)
            # print("++++++++++++++++++++++input content:", input_imgs[0])
            # print("======================label size:", label_imgs[0].shape)
            # print("++++++++++++++++++++++label content:", label_imgs[0])

            # store in GPU for CUDA processing
            if is_use_cuda and torch.cuda.is_available():
                input_imgs = input_imgs.cuda()
                label_imgs = label_imgs.cuda()


            #------------------- 1. Train Discriminator -------------------
            if (iter - 1) % (d_niter_freeze + 1) == 0:
                for _ in range(d_nupdate_per_iter):

                    # reset gradients to zero, because Pytorch accumulates the gradients on subsequent backward passes
                    d_model.zero_grad()

                    # get output images from G
                    g_output = g_model(input_imgs)
                    # map g_output image from -1 - 1 (float) to 0-255 (float)
                    g_output = (g_output * 127.5) + 127.5
                    # g_output = torch.round(g_output)
                    # print("======================g_output size:", g_output[0].shape)
                    # print("++++++++++++++++++++++g_output content:", g_output[0])

                    # 1.1 train with "real" images (data from training set)
                    d_output_real = d_model(label_imgs)
                    d_loss_real = d_criterion(d_output_real,
                                              generate_binary_label(DLabel.REAL,
                                                                    len(d_output_real),
                                                                    device))
                    d_loss_real.backward()

                    # 1.2 train with "fake" images (data outputted by G)
                    d_output_fake = d_model(g_output)
                    d_loss_fake = d_criterion(d_output_fake,
                                               generate_binary_label(DLabel.FAKE,
                                                                     len(d_output_fake),
                                                                     device))

                    d_loss_fake.backward()

                    # 1.3 calculate total D loss and update optimizer
                    d_loss = d_loss_real + d_loss_fake
                    d_optimizer.step()

            #------------------- 2. Train Generator -------------------
            if (iter - 1) % (g_niter_freeze + 1) == 0:
                for _ in range(g_nupdate_per_iter):

                    # reset gradients to zero, because Pytorch accumulates the gradients on subsequent backward passes
                    g_model.zero_grad()

                    # get output images from G
                    g_output = g_model(input_imgs)
                    # map g_output image from -1 - 1 (float) to 0-255 (float)
                    g_output = (g_output * 127.5) + 127.5
                    # g_output = torch.round(g_output)

                    # print("======================g_output1 size:", g_output[0].shape)
                    # print("++++++++++++++++++++++g_output1 content:", g_output[0])

                    # use output from D to get the loss of G
                    d_output_fake_2 = d_model(g_output)
                    g_loss = g_criterion(d_output_fake_2,
                                         generate_binary_label(DLabel.REAL,
                                                               len(d_output_fake_2),
                                                               device))
                    g_loss.backward()
                    g_optimizer.step()

            # ------------------- 3. store datapoint w/ frequency based on 'nbatches_per_dpt' and at last iteration -------------------
            if (iter - 1) % nbatches_per_dpt == 0 or iter == niter:
                # 3.1 compute and record accuracy / loss for G
                train_acc_g.append((iter, compute_G_accuracy(g_model, d_model,
                                                             train_dataset,
                                                             is_use_cuda=is_use_cuda)))
                train_loss_g.append((iter, g_loss.item()))
                val_acc_g.append((iter, compute_G_accuracy(g_model, d_model,
                                                           valid_dataset,
                                                           is_use_cuda=is_use_cuda)))
                val_loss_g.append((iter, compute_G_loss(g_model, d_model,
                                                        g_criterion,
                                                        valid_dataset,
                                                        is_use_cuda=is_use_cuda, device=device)))             

                # 3.2 compute and record accuracy / loss for D
                train_acc_d.append((iter, compute_D_accuracy(g_model, d_model,
                                                             train_dataset,
                                                             is_use_cuda=is_use_cuda)))
                train_loss_d.append((iter, d_loss.item()))
                val_acc_d.append((iter, compute_D_accuracy(g_model, d_model,
                                                           valid_dataset,
                                                           is_use_cuda=is_use_cuda)))
                val_loss_d.append((iter, compute_D_loss(g_model, d_model,
                                                        d_criterion,
                                                        valid_dataset,
                                                        is_use_cuda=is_use_cuda, device=device)))
                
                # 3.3 compute and record pixel accuracy / loss  
                px_train_loss, px_train_acc = compute_pixel_loss_accuracy(g_model,
                                                                          train_dataset,
                                                                          is_use_cuda=is_use_cuda)
                px_val_loss, px_val_acc = compute_pixel_loss_accuracy(g_model,
                                                                      valid_dataset,
                                                                      is_use_cuda=is_use_cuda)
                train_pixel_acc_g.append((iter, px_train_acc))
                train_pixel_loss_g.append((iter, px_train_loss))
                val_pixel_acc_g.append((iter, px_val_acc))
                val_pixel_loss_g.append((iter, px_val_loss))

                # ------------------- 4. output training information for every epoch -------------------
                print(f"Epoch {epoch} | iter {iter}: ")
                # a) generator
                print("a) generator")
                print(f"\t training accuracy: {train_acc_g[-1][1]}")
                print(f"\t training loss: {train_loss_g[-1][1]}")
                print(f"\t validation accuracy: {val_acc_g[-1][1]}")
                print(f"\t validation loss: {val_loss_g[-1][1]}")
                # b) discriminator
                print("b) discriminator")
                print(f"\t training accuracy: {train_acc_d[-1][1]}")
                print(f"\t training loss: {train_loss_d[-1][1]}")
                print(f"\t validation accuracy: {val_acc_d[-1][1]}")
                print(f"\t validation loss: {val_loss_d[-1][1]}")
                # c) pixel loss
                print("c) pixel loss")
                print(f"\t training accuracy: {train_pixel_acc_g[-1][1]}")
                print(f"\t training loss: {train_pixel_loss_g[-1][1]}")
                print(f"\t validation accuracy: {val_pixel_acc_g[-1][1]}")
                print(f"\t validation loss: {val_pixel_loss_g[-1][1]}")

            # ------------------- 5. plot output w/ frequency based on 'nbatches_per_plot_g_out' and at last iteration -------------------
            if (iter - 1) % nbatches_per_plot_g_out == 0 or iter == niter:
                print(f"Epoch {epoch} | iter {iter}: "
                      f"input image - output image - label image")
                fig = plt.figure(figsize=(15, 15))
                cols = 3
                rows = nimgs_per_plot_g_out

                for r in range(1, rows + 1):
                    i = (r - 1) * cols

                    # a) plot label image
                    fig.add_subplot(rows, cols, i + 1)
                    plt.imshow(np.transpose(input_imgs[r-1].detach().cpu() / 255,
                                            (1, 2, 0)))

                    # b) plot output image
                    fig.add_subplot(rows, cols, i + 2)
                    plt.imshow(np.transpose(g_output[r-1].detach().cpu() / 255,
                                            (1, 2, 0)))

                    # c) plot input image
                    fig.add_subplot(rows, cols, i + 3)
                    plt.imshow(np.transpose(label_imgs[r-1].detach().cpu() / 255,
                                            (1, 2, 0)))
                plt.show()

        # ------------------- 6. save frequency of the current model (checkpoint) to a file based on 'nepochs_per_model_save' and at last epoch------------------
        if (epoch - 1) % nepochs_per_model_save == 0 or epoch == nepochs:
            # 6.1 Generator
            path = rf"{model_dir}\{id}_G_ne{nepochs}_niter{niter}_e{epoch}_iter{iter}"
            torch.save(g_model.state_dict(), path)
            # 6.2 Discriminator
            path = rf"{model_dir}\{id}_D1_ne{nepochs}_niter{niter}_e{epoch}_iter{iter}"
            torch.save(d_model.state_dict(), path)


    # ------------------- 7. output elapsed time ------------------
    print('finished training')
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("total time elapsed: {:.2f} seconds".format(elapsed_time))


    # ------------------- 8. write the train/test loss/err into CSV file for plotting later ------------------
    # 8.1 Generator
    g_csv_paths = (rf"{model_dir}\{id}_G_train_accuracy.csv",
                   rf"{model_dir}\{id}_G_train_loss.csv",
                   rf"{model_dir}\{id}_G_val_accuracy.csv",
                   rf"{model_dir}\{id}_G_val_loss.csv")
    np.savetxt(g_csv_paths[0], train_acc_g, delimiter=',')
    np.savetxt(g_csv_paths[1], train_loss_g, delimiter=',')
    np.savetxt(g_csv_paths[2], val_acc_g, delimiter=',')
    np.savetxt(g_csv_paths[3], val_loss_g, delimiter=',')
    # 8.2 Discriminator
    d1_csv_paths = (rf"{model_dir}\{id}_D_train_accuracy.csv",
                    rf"{model_dir}\{id}_D_train_loss.csv",
                    rf"{model_dir}\{id}_D_val_accuracy.csv",
                    rf"{model_dir}\{id}_D_val_loss.csv")
    np.savetxt(d1_csv_paths[0], train_acc_d, delimiter=',')
    np.savetxt(d1_csv_paths[1], train_loss_d, delimiter=',')
    np.savetxt(d1_csv_paths[2], val_acc_d, delimiter=',')
    np.savetxt(d1_csv_paths[3], val_loss_d, delimiter=',')
    # 8.3 Pixel loss
    px_csv_paths = (rf"{model_dir}\{id}_PX_train_accuracy.csv",
                    rf"{model_dir}\{id}_PX_train_loss.csv",
                    rf"{model_dir}\{id}_PX_val_accuracy.csv",
                    rf"{model_dir}\{id}_PX_val_loss.csv")
    np.savetxt(px_csv_paths[0], train_pixel_acc_g, delimiter=',')
    np.savetxt(px_csv_paths[1], train_pixel_loss_g, delimiter=',')
    np.savetxt(px_csv_paths[2], val_pixel_acc_g, delimiter=',')
    np.savetxt(px_csv_paths[3], val_pixel_loss_g, delimiter=',')

    return g_csv_paths, d1_csv_paths, px_csv_paths

"""### Tunning

#### Full dataset
"""

g_model = UNetGenerator()
path_g = "/content/drive/MyDrive/ECE496 Capstone/Implementation ----- Code/final_model/test_v6_full_dataset_g_lr_0.000655_d_lr_0.0004_G_ne6000_niter102000_e2751_iter46767"
state = torch.load(path_g)
g_model.load_state_dict(state)

d_model = Discriminator()
path_d = "/content/drive/MyDrive/ECE496 Capstone/Implementation ----- Code/final_model/test_v6_full_dataset_g_lr_0.000655_d_lr_0.0004_D1_ne6000_niter102000_e2751_iter46767"
state = torch.load(path_d)
d_model.load_state_dict(state)

# g_model.apply(weights_init)
# d_model.apply(weights_init)

is_use_cuda = True
if is_use_cuda and torch.cuda.is_available():
    g_model.cuda()
    d_model.cuda()


bs = 64
t_ind = 0.8
v_ind = 0.1
o_num = 1

train_dataset, valid_dataset, test_dataset, overfit_dataset = data_loader(bs = bs, t_ind = t_ind, v_ind = v_ind, o_num = o_num, dataset = dataset_combined)

# dataiter = iter(overfit_dataset)
# sample = dataiter.next()
# sample1 = sample[0][0]
# sample2 = sample[1][0]

# plt.imshow(sample1.permute(1, 2, 0) / 255)
# plt.imshow(sample2.permute(1, 2, 0))

nepochs = 5000
id = "test_v6_full_dataset_g_lr_0.0006_d_lr_0.0005"

g_niter_freeze = 0
d_niter_freeze = 0
g_nupdate_per_iter = 1
d_nupdate_per_iter = 1

adam_g_lrate = 0.0005
adam_g_betas = None
adam_g_eps = None
adam_g_weight_decay = None
adam_g_amsgrad = None

adam_d_lrate = 0.0003
adam_d_betas = None
adam_d_eps = None
adam_d_weight_decay = None
adam_d_amsgrad = None

nbatches_per_dpt = 1000
nbatches_per_plot_g_out = 1000
nepochs_per_model_save = 250
nimgs_per_plot_g_out = 2

model_dir = '/content/drive/MyDrive/ECE496 Capstone/Implementation ----- Code/Kevin/Models/DCGAN_V8_final2/'

csv_paths = train_dcgan(g_model, d_model,
                        train_dataset, valid_dataset, nepochs, id,
                        g_niter_freeze=g_niter_freeze, d_niter_freeze=d_niter_freeze,
                        g_nupdate_per_iter=g_nupdate_per_iter,
                        d_nupdate_per_iter=d_nupdate_per_iter,
                        adam_g_lrate=adam_g_lrate, adam_g_betas=adam_g_betas, adam_g_eps=adam_g_eps,
                        adam_g_weight_decay=adam_g_weight_decay, adam_g_amsgrad=adam_g_amsgrad,
                        adam_d_lrate=adam_d_lrate, adam_d_betas=adam_d_betas, adam_d_eps=adam_d_eps,
                        adam_d_weight_decay=adam_d_weight_decay, adam_d_amsgrad=adam_d_amsgrad,
                        nbatches_per_dpt=nbatches_per_dpt,
                        nbatches_per_plot_g_out=nbatches_per_plot_g_out,
                        nepochs_per_model_save=nepochs_per_model_save,
                        nimgs_per_plot_g_out=nimgs_per_plot_g_out,
                        model_dir=model_dir,
                        is_use_cuda=is_use_cuda)
g_csv_paths, d_csv_paths, px_csv_path = csv_paths

print("training curves - generator")
plot_training_curves(*g_csv_paths)
print('')
print("training curves - discriminator")
plot_training_curves(*d_csv_paths)
print('')
print("training curves - pixel difference")
plot_training_curves(*px_csv_path)